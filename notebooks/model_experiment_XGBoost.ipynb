{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPpSr8mgU9fx",
        "outputId": "79ef7ebd-5d43-4ac8-930f-15ba8ca2e1e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "FOLDERNAME = '/content/drive/MyDrive/ML_final_project'\n",
        "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
        "\n",
        "DATAPATH = f'{FOLDERNAME}/data/'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0angaz6T66s",
        "outputId": "731de15a-cc67-47b6-9685-e4869ee1d8d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m119.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.3/26.3 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m98.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.7/242.7 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.4/84.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m733.8/733.8 kB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m508.0/508.0 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install numpy==1.26.4 --force-reinstall -q\n",
        "!pip install mlflow==2.15.1 -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZH9V1_yXDX_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import xgboost as xgb\n",
        "#import optuna\n",
        "import joblib\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "import mlflow.xgboost\n",
        "from sklearn.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDQ1gkxg6HX8"
      },
      "outputs": [],
      "source": [
        "!pip install dagshub mlflow -q\n",
        "\n",
        "\n",
        "import dagshub\n",
        "import mlflow\n",
        "\n",
        "\n",
        "dagshub.init(repo_owner='skara-21', repo_name='ML_Final_Project', mlflow=True)\n",
        "mlflow.set_tracking_uri('https://dagshub.com/skara-21/ML_Final_Project.mlflow')\n",
        "\n",
        "try:\n",
        "    experiment_id = mlflow.create_experiment(\"XGBoost_Training\")\n",
        "    print(f\"Created new experiment: XGBoost_Training\")\n",
        "except mlflow.exceptions.MlflowException:\n",
        "    experiment = mlflow.get_experiment_by_name(\"XGBoost_Training\")\n",
        "    experiment_id = experiment.experiment_id\n",
        "    print(f\"Using existing experiment: XGBoost_Training\")\n",
        "\n",
        "mlflow.set_experiment(\"XGBoost_Training\")\n",
        "\n",
        "print(\" MLflow setup complete\")\n",
        "print(\" Your experiments will be visible at:\")\n",
        "print(\"   https://dagshub.com/kechik21/ML_Final_Project\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Os7I0AsXgwh8"
      },
      "source": [
        "# **MLflow Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eKZjl0BfgtMk"
      },
      "outputs": [],
      "source": [
        "#mlflow.set_tracking_uri(\"file:///content/drive/MyDrive/ML_final_project/mlruns\")\n",
        "#experiment_name = \"XGBoost_Training\"\n",
        "\n",
        "# Create or get experiment\n",
        "#try:\n",
        " #   experiment_id = mlflow.create_experiment(experiment_name)\n",
        " #   print(f\"Created new experiment: {experiment_name}\")\n",
        "#except mlflow.exceptions.MlflowException:\n",
        " #   experiment = mlflow.get_experiment_by_name(experiment_name)\n",
        " #   experiment_id = experiment.experiment_id\n",
        " #   print(f\"Using existing experiment: {experiment_name}\")\n",
        "\n",
        "#mlflow.set_experiment(experiment_name)\n",
        "#print(f\"Experiment ID: {experiment_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwq0I7dshP9D"
      },
      "source": [
        "**Initial Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tyl-D0u9Yd8d"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv(DATAPATH + 'train.csv')\n",
        "test_df = pd.read_csv(DATAPATH + 'test.csv')\n",
        "features_df = pd.read_csv(DATAPATH + 'features.csv')\n",
        "stores_df = pd.read_csv(DATAPATH + 'stores.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5V9EO-wPYxK1"
      },
      "outputs": [],
      "source": [
        "print(f\"Train Shape : {train_df.shape}\")\n",
        "print(f\"Train Shape : {test_df.shape}\")\n",
        "print(f\"Train Shape : {features_df.shape}\")\n",
        "print(f\"Train Shape : {stores_df.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EpCn4TBh8Gu"
      },
      "source": [
        "დავლოგავ ჩემი საწყისი დეითას ზომებს, რათა შემდეგ ქლინინგის შედეგად მიღებულს შევადარო"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pPmv5AZzhVL9"
      },
      "outputs": [],
      "source": [
        "with mlflow.start_run(run_name= \"XGBoost_Data_Loading\") as run:\n",
        "  mlflow.log_metric(\"initial_train_rows\", train_df.shape[0])\n",
        "  mlflow.log_metric(\"initial_train_cols\", train_df.shape[1])\n",
        "  mlflow.log_metric(\"initial_test_rows\", test_df.shape[0])\n",
        "  mlflow.log_metric(\"initial_test_cols\", test_df.shape[1])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tf0DqsKuRZW8"
      },
      "source": [
        "# **Cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmVhNDoQY0HC"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime, timedelta\n",
        "#იმისთვის, რომ ბევრად მარტივი გახდეს მუშაობა და\n",
        "#ადვილად შევძლოთ დროსთან კავშირი ,გადავიყვანოთ\n",
        "#object ---> datetime\n",
        "\n",
        "def object_to_datetime(input, col = 'Data', curr_part=''):\n",
        "  before_type = input[col].dtype\n",
        "  input[col]=pd.to_datetime(input[col])\n",
        "  after_type = input[col].dtype\n",
        "  print(f\"Converting data types for {curr_part}\" )\n",
        "  print(f\"Before - {before_type}\")\n",
        "  print(f\"After - {after_type}\")\n",
        "  print(\" \")\n",
        "  return input\n",
        "\n",
        "train_df = object_to_datetime(train_df, col='Date', curr_part='Train data')\n",
        "test_df = object_to_datetime(test_df, col='Date', curr_part='Test data')\n",
        "features_df = object_to_datetime(features_df, col='Date', curr_part='Features data')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oDuuQBwDhNfQ"
      },
      "outputs": [],
      "source": [
        "#დამატებითი დეითას მერჯინგი ტესტთან და ტრეინთან\n",
        "train_merged = train_df.merge(stores_df, on='Store', how='left')\n",
        "test_merged = test_df.merge(stores_df, on='Store', how='left')\n",
        "\n",
        "#features-ის დამატება\n",
        "train_merged = train_merged.merge(features_df, on=['Store', 'Date'], how='left')\n",
        "test_merged = test_merged.merge(features_df, on=['Store', 'Date'], how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7K_aM7IG7PnW"
      },
      "outputs": [],
      "source": [
        "def group_columns_by_dtype(df):\n",
        "    data_types = df.dtypes.unique()\n",
        "    for dtype in data_types:\n",
        "        cols = df.select_dtypes(include=[dtype]).columns.tolist()\n",
        "        print(f\"\\n{dtype} columns:\")\n",
        "        for col in cols:\n",
        "            print(f\" * {col}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlO0OnHF8jVn"
      },
      "outputs": [],
      "source": [
        "group_columns_by_dtype(train_merged)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULAm6xTbBM4A"
      },
      "outputs": [],
      "source": [
        "def print_columns_info():\n",
        "    print(\"Feature Columns:\")\n",
        "    print(\"   \", features_df.columns.tolist(), \"\\n\")\n",
        "\n",
        "    print(\"Store Columns:\")\n",
        "    print(\"   \", stores_df.columns.tolist(), \"\\n\")\n",
        "\n",
        "    print(\"Train Columns:\")\n",
        "    print(\"   Before merging:\", train_df.columns.tolist())\n",
        "    print(\"   After  merging:\", train_merged.columns.tolist(), \"\\n\")\n",
        "\n",
        "    print(\"Test Columns:\")\n",
        "    print(\"   Before merging:\", test_df.columns.tolist())\n",
        "    print(\"   After  merging:\", test_merged.columns.tolist(), \"\\n\")\n",
        "\n",
        "\n",
        "print_columns_info()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTCZGWghD2CZ"
      },
      "source": [
        "გაგვიჩნდა პატარა პრობლემა, მერჯინგის დროს გაჩნდა IsHoliday_x & IsHoliday_y , ამიტომაც გავარკვიოთ რომლის დატოვე სჯობს"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9Fd7a6ZF1BD"
      },
      "outputs": [],
      "source": [
        "same_cols = (train_merged['IsHoliday_x'] == train_merged['IsHoliday_y']).all()\n",
        "print(\"Are they the same? \", same_cols)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSJa-1K-AbFE"
      },
      "outputs": [],
      "source": [
        "train_merged['IsHoliday'] = train_merged['IsHoliday_y'].fillna(train_merged['IsHoliday_x'])\n",
        "test_merged['IsHoliday'] = test_merged['IsHoliday_y'].fillna(test_merged['IsHoliday_x'])\n",
        "\n",
        "train_merged = train_merged.drop(['IsHoliday_x', 'IsHoliday_y'], axis=1)\n",
        "test_merged = test_merged.drop(['IsHoliday_x', 'IsHoliday_y'], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42wZzy2okWPg"
      },
      "source": [
        "რადგანაც Weekly Sales ზოგიერთგან უარყოფითი გვაქვს, მოვიშოროთ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkNgZoqhj3-J"
      },
      "outputs": [],
      "source": [
        "init_rows = len(train_merged)\n",
        "train_merged = train_merged[train_merged['Weekly_Sales'] >=0 ]\n",
        "removed_ones = init_rows - len(train_merged)\n",
        "print(f\"Removed negative values are {removed_ones}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwvhAnsEkgRZ"
      },
      "outputs": [],
      "source": [
        "with mlflow.start_run(run_name=\"XGBoost_Cleaning\") as run:\n",
        "  mlflow.log_metric(\"negative_saled_removed\",removed_ones)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4TGjmnGzjOV"
      },
      "source": [
        "**Finding Missing Values**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8XPgF1YzXcL"
      },
      "outputs": [],
      "source": [
        "def check_missing(input, name=\"DataFrame\"):\n",
        "    missing_amount = input.isnull().sum()\n",
        "    really_missing = missing_amount[missing_amount > 0]\n",
        "\n",
        "    if not really_missing.empty:\n",
        "        print(f\" Missing values in {name}:\")\n",
        "        print(really_missing)\n",
        "        return really_missing.to_dict()\n",
        "    else:\n",
        "        print(f\"No missing values\")\n",
        "        return {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyXN1iaArfgB"
      },
      "outputs": [],
      "source": [
        "with mlflow.start_run(run_name=\"XGBoost_Missing_values\") as run:\n",
        "   train_missing = check_missing(train_merged, \"Train Merged\")\n",
        "   test_missing = check_missing(test_merged, \"Test Merged\")\n",
        "\n",
        "   #დავლოგოთ მისინგები\n",
        "   for col, count in train_missing.items():\n",
        "        mlflow.log_metric(f\"train_missing_{col}\", count)\n",
        "   for col, count in test_missing.items():\n",
        "        mlflow.log_metric(f\"test_missing_{col}\", count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "br7MAeaRWx0v"
      },
      "outputs": [],
      "source": [
        "check_missing(train_merged, \"Train Merged\")\n",
        "check_missing(test_merged, \"Test Merged\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FaLtTJOcChJ"
      },
      "outputs": [],
      "source": [
        "num_cols = ['CPI','Fuel_Price','Unemployment','Temperature']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8NlmxIccPIW"
      },
      "outputs": [],
      "source": [
        "def fill_numericals(num_cols,train_merged,test_merged):\n",
        "  for col in num_cols:\n",
        "    if col in train_merged.columns:\n",
        "        train_missing = train_merged[col].isnull().sum()\n",
        "        if train_missing > 0:\n",
        "            print(f\"Filling {train_missing} missing values in {col} (train)\")\n",
        "            train_merged[col] = train_merged.groupby('Store')[col].ffill()\n",
        "            train_merged[col] = train_merged[col].fillna(train_merged[col].median())\n",
        "\n",
        "\n",
        "        test_missing = test_merged[col].isnull().sum()\n",
        "        if test_missing > 0:\n",
        "            print(f\"Filling {test_missing} missing values in {col} (test)\")\n",
        "            test_merged[col] = test_merged.groupby('Store')[col].ffill()\n",
        "            test_merged[col] = test_merged[col].fillna(test_merged[col].median())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-KDmG9VcavT"
      },
      "outputs": [],
      "source": [
        "fill_numericals(num_cols,train_merged,test_merged)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70fVApptWUWT"
      },
      "source": [
        "რადგანაც Mark Downs გვაქვს მხოლოდ missing data-თი, შევავსოთ ნულებით"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCmivTDj2MLx"
      },
      "outputs": [],
      "source": [
        "markdowns = [col for col in train_merged.columns if 'MarkDown' in col]\n",
        "for col in markdowns:\n",
        "    train_merged[col] = train_merged[col].fillna(0)\n",
        "    test_merged[col] = test_merged[col].fillna(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tkwdfrv5jYi_"
      },
      "source": [
        "ყოველი შემთხვევისთვის, isHoliday-ც გავითვალისწინოთ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0pWfalx_Cvo"
      },
      "outputs": [],
      "source": [
        "train_merged['IsHoliday'] = train_merged['IsHoliday'].fillna(False)\n",
        "test_merged['IsHoliday'] = test_merged['IsHoliday'].fillna(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdxgBKb3rg0O"
      },
      "source": [
        "Markdown-ების ნაწილი, რომელიც შედარებით ცუდია, კერძოს 1/3/5 მოვიშოროთ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RiThJ0KitN7t"
      },
      "outputs": [],
      "source": [
        "removed_cols = ['MarkDown1','MarkDown3','MarkDown5']\n",
        "mlflow.log_param(\"removed_cols\",removed_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLIG7sc-0bUP"
      },
      "outputs": [],
      "source": [
        "train_merged = train_merged.drop('MarkDown1', axis=1)\n",
        "train_merged = train_merged.drop('MarkDown3', axis=1)\n",
        "train_merged = train_merged.drop('MarkDown5', axis=1)\n",
        "test_merged = test_merged.drop('MarkDown1', axis=1)\n",
        "test_merged = test_merged.drop('MarkDown3', axis=1)\n",
        "test_merged = test_merged.drop('MarkDown5', axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uz20fXSNl4-p"
      },
      "source": [
        "# **Feature Engineering**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ko7ltqyT_G7l"
      },
      "outputs": [],
      "source": [
        "def additional_time_feat(input):\n",
        "  input = input.copy()\n",
        "  input['Day'] = input['Date'].dt.day.astype('int64')\n",
        "  input['Week'] = input['Date'].dt.isocalendar().week.astype('int64')\n",
        "  input['Month'] = input['Date'].dt.month.astype('int64')\n",
        "  input['Year'] = input['Date'].dt.year.astype('int64')\n",
        "  const_month = 2 * np.pi * input['Month']\n",
        "  const_week = 2 * np.pi * input['Week']\n",
        "  input['Month_sin'] = np.sin( const_month/ 12).astype('float64')\n",
        "  input['Month_cos'] = np.cos(const_month / 12).astype('float64')\n",
        "  input['Week_sin'] = np.sin( const_week/ 52).astype('float64')\n",
        "  input['Week_cos'] = np.cos(const_week/ 52).astype('float64')\n",
        "\n",
        "  return input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39dvS0kSpyIw"
      },
      "outputs": [],
      "source": [
        "def additional_cat_feat(input):\n",
        "  input= input.copy()\n",
        "  label_enc_type = LabelEncoder()\n",
        "  input['Type_le'] = label_enc_type.fit_transform(input['Type'])\n",
        "  return input, label_enc_type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XLU3pZL5wE9h"
      },
      "outputs": [],
      "source": [
        "def additional_month_feat_train(input, col='Weekly_Sales'):\n",
        "    input = input.copy()\n",
        "    input = input.reset_index(drop=True)\n",
        "    input = input.sort_values(['Store','Dept','Date'])\n",
        "    print(f\"Additional features are being added for {col}\")\n",
        "\n",
        "    my_w = [4, 8, 12]\n",
        "\n",
        "    for w in my_w:\n",
        "        print(f\"Adding {w} week rolling mean features\")\n",
        "        added_mn = []\n",
        "        for (store, dept), group in input.groupby(['Store', 'Dept']):\n",
        "            group = group.sort_values('Date').reset_index(drop=True)\n",
        "            month_mean = group[col].rolling(w, min_periods=1).mean()\n",
        "            added_mn.extend(month_mean.values)\n",
        "        input = input.sort_values(['Store', 'Dept', 'Date']).reset_index(drop=True)\n",
        "        input[f'{col}_month_mean_{w}'] = added_mn\n",
        "        print(f\"Added column: {col}_month_mean_{w}\")\n",
        "    print(\"All addition is done\")\n",
        "    return input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6iA9SEXCsMFJ"
      },
      "outputs": [],
      "source": [
        "def additional_month_feat_test(train_data, test_input, col='Weekly_Sales'):\n",
        "    test_input = test_input.copy()\n",
        "    print(f\"Adding rolling features to test data depending on {col}\")\n",
        "    month_cols = [c for c in train_data.columns if f'{col}_month_mean_' in c]\n",
        "    for month_col in month_cols:\n",
        "        print(f\"Adding {month_col} to test data\")\n",
        "        store_dept_avg = train_data.groupby(['Store', 'Dept'])[month_col].mean().reset_index()\n",
        "        store_dept_avg.columns = ['Store', 'Dept', month_col]\n",
        "        test_input = test_input.merge(store_dept_avg, on=['Store', 'Dept'], how='left')\n",
        "        overall_avg = train_data[month_col].mean()\n",
        "        test_input[month_col] = test_input[month_col].fillna(overall_avg)\n",
        "\n",
        "    print(\"All rolling features are complete\")\n",
        "    return test_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytM9XZUmyCcJ"
      },
      "outputs": [],
      "source": [
        "mlflow.end_run()\n",
        "\n",
        "with mlflow.start_run(run_name=\"XGBoost_Feature_Engineering\") as run:\n",
        "  train_merged= additional_time_feat(train_merged)\n",
        "  test_merged = additional_time_feat(test_merged)\n",
        "  train_merged,le_1= additional_cat_feat(train_merged)\n",
        "  test_merged['Type_le'] = le_1.transform(test_merged['Type'])\n",
        "  train_merged = additional_month_feat_train(train_merged, col='Weekly_Sales')\n",
        "  test_merged = additional_month_feat_test(train_merged, test_merged, col='Weekly_Sales')\n",
        "\n",
        "  mlflow.log_param(\"time_features_added\", [\"Day\", \"Week\", \"Month\", \"Year\", \"Month_sin\", \"Month_cos\", \"Week_sin\", \"Week_cos\"])\n",
        "  mlflow.log_param(\"rolling_windows\", [4, 8, 12])\n",
        "  mlflow.log_param(\"categorical_encoding\", \"LabelEncoder\")\n",
        "\n",
        "\n",
        "  print(f\"Final train shape is : {train_merged.shape}\")\n",
        "  print(f\"Final test shape is : {test_merged.shape}\")\n",
        "\n",
        "  mlflow.log_metric(\"final_train_features\", train_merged.shape[1])\n",
        "  mlflow.log_metric(\"final_test_features\", test_merged.shape[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uJRNqtzuRJL"
      },
      "source": [
        "# **Pipeline Creation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAbzhEsE2Qc-"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "import mlflow.xgboost\n",
        "from sklearn.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8y0zK1ruVFh"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class WalmartDataPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.label_encoder = None\n",
        "        self.train_data_for_rolling = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.label_encoder = le_1\n",
        "        self.train_data_for_rolling = train_merged\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_processed = X.copy()\n",
        "        X_processed = X_processed.merge(stores_df, on='Store', how='left')\n",
        "        X_processed = X_processed.merge(features_df, on=['Store', 'Date'], how='left')\n",
        "\n",
        "\n",
        "        if 'IsHoliday_x' in X_processed.columns and 'IsHoliday_y' in X_processed.columns:\n",
        "            X_processed['IsHoliday'] = X_processed['IsHoliday_y'].fillna(X_processed['IsHoliday_x'])\n",
        "            X_processed = X_processed.drop(['IsHoliday_x', 'IsHoliday_y'], axis=1)\n",
        "\n",
        "\n",
        "        X_processed = object_to_datetime(X_processed, col='Date', curr_part='Pipeline transform')\n",
        "\n",
        "\n",
        "        num_cols = ['CPI', 'Fuel_Price', 'Unemployment', 'Temperature']\n",
        "        fill_numericals(num_cols, X_processed, X_processed)  # Just pass same df twice\n",
        "\n",
        "\n",
        "        markdowns = [col for col in X_processed.columns if 'MarkDown' in col]\n",
        "        for col in markdowns:\n",
        "            X_processed[col] = X_processed[col].fillna(0)\n",
        "        X_processed['IsHoliday'] = X_processed['IsHoliday'].fillna(False)\n",
        "        removed_cols = ['MarkDown1', 'MarkDown3', 'MarkDown5']\n",
        "        X_processed = X_processed.drop([col for col in removed_cols if col in X_processed.columns], axis=1)\n",
        "        X_processed = additional_time_feat(X_processed)\n",
        "        X_processed['Type_le'] = self.label_encoder.transform(X_processed['Type'])\n",
        "        X_processed = additional_month_feat_test(self.train_data_for_rolling, X_processed, col='Weekly_Sales')\n",
        "        cols_to_remove = ['Date', 'Type']\n",
        "        if 'Weekly_Sales' in X_processed.columns:\n",
        "            cols_to_remove.append('Weekly_Sales')\n",
        "        X_processed = X_processed.drop([col for col in cols_to_remove if col in X_processed.columns], axis=1)\n",
        "\n",
        "        return X_processed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecLsA80Y1YkK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPRqp6mI1ZKh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDdk29m1QyLo"
      },
      "source": [
        "# **Cross Validation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eAHFT6fCspT"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wES4BPRb54vt"
      },
      "outputs": [],
      "source": [
        "def run_cross_validation(X_train, y_train):\n",
        "    \"\"\"Complete your cross validation function\"\"\"\n",
        "\n",
        "    with mlflow.start_run(run_name=\"XGBoost_Cross_Validation\") as run:\n",
        "        tscv = TimeSeriesSplit(n_splits=5)\n",
        "        cv_scores = []\n",
        "\n",
        "        base_params = {\n",
        "            'objective': 'reg:squarederror',\n",
        "            'max_depth': 6,\n",
        "            'learning_rate': 0.1,\n",
        "            'n_estimators': 1000,\n",
        "            'subsample': 0.8,\n",
        "            'colsample_bytree': 0.8,\n",
        "            'random_state': 42,\n",
        "            'n_jobs': -1\n",
        "        }\n",
        "        for param, value in base_params.items():\n",
        "            mlflow.log_param(f\"cv_{param}\", value)\n",
        "\n",
        "        for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train)):\n",
        "            print(f\"Working on fold {fold + 1}/5\")\n",
        "\n",
        "            X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "            y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "            model = xgb.XGBRegressor(**base_params)\n",
        "            model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)\n",
        "\n",
        "            y_pred = model.predict(X_val)\n",
        "            mae = mean_absolute_error(y_val, y_pred)\n",
        "            rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "            r2 = r2_score(y_val, y_pred)\n",
        "\n",
        "            cv_scores.append({'fold': fold, 'mae': mae, 'rmse': rmse, 'r2': r2})\n",
        "\n",
        "            # Log metrics\n",
        "            mlflow.log_metric(f\"fold_{fold}_mae\", mae)\n",
        "            mlflow.log_metric(f\"fold_{fold}_rmse\", rmse)\n",
        "            mlflow.log_metric(f\"fold_{fold}_r2\", r2)\n",
        "\n",
        "            print(f\"  MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.4f}\")\n",
        "        avg_mae = np.mean([score['mae'] for score in cv_scores])\n",
        "        avg_rmse = np.mean([score['rmse'] for score in cv_scores])\n",
        "        avg_r2 = np.mean([score['r2'] for score in cv_scores])\n",
        "\n",
        "        mlflow.log_metric(\"cv_avg_mae\", avg_mae)\n",
        "        mlflow.log_metric(\"cv_avg_rmse\", avg_rmse)\n",
        "        mlflow.log_metric(\"cv_avg_r2\", avg_r2)\n",
        "\n",
        "        print(f\"\\\\nCross Validation Results:\")\n",
        "        print(f\"Average MAE: {avg_mae:.2f}\")\n",
        "        print(f\"Average RMSE: {avg_rmse:.2f}\")\n",
        "        print(f\"Average R2: {avg_r2:.4f}\")\n",
        "\n",
        "        return cv_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhW06DF7Rskz"
      },
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIwSyQbvW-0Z"
      },
      "outputs": [],
      "source": [
        "def run_final_training(X_train, y_train):\n",
        "    \"\"\"Complete your final training function\"\"\"\n",
        "\n",
        "    with mlflow.start_run(run_name=\"XGBoost_Final_Training\") as run:\n",
        "\n",
        "        final_params = {\n",
        "            'objective': 'reg:squarederror',\n",
        "            'max_depth': 6,\n",
        "            'learning_rate': 0.1,\n",
        "            'n_estimators': 1000,\n",
        "            'subsample': 0.8,\n",
        "            'colsample_bytree': 0.8,\n",
        "            'random_state': 42,\n",
        "            'n_jobs': -1\n",
        "        }\n",
        "\n",
        "\n",
        "        for param, value in final_params.items():\n",
        "            mlflow.log_param(param, value)\n",
        "\n",
        "\n",
        "        final_model = xgb.XGBRegressor(**final_params)\n",
        "        final_model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "        pipeline = Pipeline([\n",
        "            ('preprocessor', WalmartDataPreprocessor()),\n",
        "            ('model', xgb.XGBRegressor(**final_params))\n",
        "        ])\n",
        "        original_X = train_df.drop('Weekly_Sales', axis=1)\n",
        "        original_y = train_df['Weekly_Sales']\n",
        "        pipeline.fit(original_X, original_y)\n",
        "        mlflow.sklearn.log_model(\n",
        "            pipeline,\n",
        "            \"xgboost_pipeline\",\n",
        "            registered_model_name=\"walmart_xgboost_pipeline\"\n",
        "        )\n",
        "\n",
        "        mlflow.xgboost.log_model(\n",
        "            final_model,\n",
        "            \"xgboost_model\",\n",
        "            registered_model_name=\"walmart_xgboost_model\"\n",
        "        )\n",
        "        #mlflow.sklearn.log_model(pipeline, \"xgboost_pipeline\")\n",
        "        #mlflow.xgboost.log_model(final_model, \"xgboost_model\")\n",
        "\n",
        "\n",
        "        train_pred = final_model.predict(X_train)\n",
        "        mae = mean_absolute_error(y_train, train_pred)\n",
        "        rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n",
        "        r2 = r2_score(y_train, train_pred)\n",
        "        mlflow.log_metric(\"final_mae\", mae)\n",
        "        mlflow.log_metric(\"final_rmse\", rmse)\n",
        "        mlflow.log_metric(\"final_r2\", r2)\n",
        "\n",
        "        print(f\"Final model metrics:\")\n",
        "        print(f\"MAE: {mae:.2f}, RMSE: {rmse:.2f}, R2: {r2:.4f}\")\n",
        "\n",
        "        return pipeline, final_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q54dT502KFe"
      },
      "source": [
        "# **Main Execution**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mqfjTyy1731"
      },
      "outputs": [],
      "source": [
        "\n",
        "feature_cols = [col for col in train_merged.columns if col not in ['Weekly_Sales', 'Date', 'Type']]\n",
        "removed_cols = ['MarkDown1', 'MarkDown3', 'MarkDown5']\n",
        "feature_cols = [col for col in feature_cols if col not in removed_cols]\n",
        "\n",
        "X_train = train_merged[feature_cols].copy()\n",
        "y_train = train_merged['Weekly_Sales'].copy()\n",
        "\n",
        "print(f\"\\\\nTraining with {len(feature_cols)} features using your processed data\")\n",
        "print(f\"Training data shape: {X_train.shape}\")\n",
        "\n",
        "print(\"RUNNING CROSS VALIDATION\")\n",
        "\n",
        "cv_scores = run_cross_validation(X_train, y_train)\n",
        "print(\"TRAINING FINAL MODEL\")\n",
        "pipeline, model = run_final_training(X_train, y_train)\n",
        "\n",
        "print(\"GENERATING PREDICTIONS\")\n",
        "\n",
        "\n",
        "test_feature_cols = [col for col in feature_cols if col in test_merged.columns]\n",
        "X_test_processed = test_merged[test_feature_cols].copy()\n",
        "for col in feature_cols:\n",
        "    if col not in X_test_processed.columns:\n",
        "        print(f\"Adding missing column {col} with default value 0\")\n",
        "        X_test_processed[col] = 0\n",
        "X_test_processed = X_test_processed[feature_cols]\n",
        "test_predictions = model.predict(X_test_processed)\n",
        "\n",
        "print(f\"Generated {len(test_predictions)} test predictions\")\n",
        "print(\" XGBoost model complete with pipeline!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBCTH5ft2C5i"
      },
      "source": [
        "# **Submission File**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oXOqqKD19M1"
      },
      "outputs": [],
      "source": [
        "with mlflow.start_run(run_name=\"XGBoost_Submission_Generation\") as run:\n",
        "    print(\"\\\\n\" + \"=\"*50)\n",
        "    print(\"CREATING SUBMISSION FILE\")\n",
        "    print(\"=\"*50)\n",
        "    test_submission = test_merged[['Store', 'Dept', 'Date']].copy()\n",
        "    test_submission['Weekly_Sales'] = test_predictions\n",
        "    test_submission['Id'] = (test_submission['Store'].astype(str) + '_' +\n",
        "                           test_submission['Dept'].astype(str) + '_' +\n",
        "                           test_submission['Date'].dt.strftime('%Y-%m-%d'))\n",
        "\n",
        "    submission = test_submission[['Id', 'Weekly_Sales']].copy()\n",
        "    submission_path = f\"{FOLDERNAME}/xgboost_submission.csv\"\n",
        "    submission.to_csv(submission_path, index=False)\n",
        "\n",
        "\n",
        "    mlflow.log_artifact(submission_path)\n",
        "    mlflow.log_metric(\"submission_rows\", len(submission))\n",
        "\n",
        "    print(f\"Submission saved to: {submission_path}\")\n",
        "    print(f\"Submission shape: {submission.shape}\")\n",
        "    print(\"\\\\nFirst 10 rows of submission:\")\n",
        "    print(submission.head(10))\n",
        "\n",
        "    print(\" XGBOOST EXPERIMENT COMPLETED SUCCESSFULLY!\")\n",
        "    print(\" All MLflow runs logged\")\n",
        "    print(\" Model registered in Model Registry\")\n",
        "    print(\" Pipeline ready for deployment\")\n",
        "    print(\" Submission file generated\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
